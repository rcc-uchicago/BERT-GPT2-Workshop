{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pytorch-pretrained-bert in /Users/tharsen/anaconda3/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied, skipping upgrade: torch>=0.4.1 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.91)\n",
      "Requirement already satisfied, skipping upgrade: regex in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
      "Requirement already satisfied, skipping upgrade: future in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2018.4.16)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.91 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.91)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.91->boto3->pytorch-pretrained-bert) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.91->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/tharsen/anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.91->boto3->pytorch-pretrained-bert) (1.15.0)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '[CLS] I want to buy the jacket because it is [MASK] . [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/tharsen/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Create the segments tensors.\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/tharsen/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/tharsen/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/kc/pxfdnv096hl6g4km_vmh63c80000gn/T/tmp1kfmf8ha\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "masked_index = tokenized_text.index('[MASK]') \n",
    "\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "#predicted_index = torch.argmax(predictions[0][0][masked_index]).item()\n",
    "\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
